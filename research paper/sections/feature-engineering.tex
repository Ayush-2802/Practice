\section{Feature Engineering for Industrial Sensor Data}
\subsection{Domain-Specific Feature Extraction}
Our feature engineering process incorporated specialized transformations particularly relevant to slitting machinery:

\subsubsection{Spectral Features}
We extracted rich frequency-domain information through:
\begin{itemize}
    \item Fast Fourier Transform (FFT) with specifically designed frequency bands corresponding to known mechanical resonances
    \item Wavelet transforms using Daubechies wavelets (specifically DB4) for multi-resolution analysis
    \item Spectral entropy measures that quantified the complexity of vibration signals
    \item Mel-frequency cepstral coefficients (MFCCs) adapted for machine acoustic signatures
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/spectral-features.pdf}
\caption{Spectral feature extraction process showing FFT transformation of vibration data and identification of key frequency bands associated with different failure modes.}
\label{fig:spectral_features}
\end{figure}

Spectral features proved particularly valuable for identifying bearing issues, with specific frequency bands (2.1-2.5 kHz) showing strong correlation with early-stage bearing degradation.

\subsubsection{Multi-scale Statistical Features}
We computed statistical measures across multiple time windows (1s, 10s, 1m, 10m):
\begin{itemize}
    \item Rolling quantiles (5\%, 25\%, 50\%, 75\%, 95\%) capturing distributional changes
    \item Kurtosis and skewness for detecting subtle shifts in signal distribution
    \item Hurst exponent for quantifying long-term persistence in measurements
    \item Allan variance specifically for gyroscopic stability analysis
\end{itemize}

Multi-scale analysis revealed that tension variance in the 10s window provided the earliest indication of material feeding issues (average lead time: 37 minutes).

\subsubsection{Phase Space Reconstruction}
We employed techniques from nonlinear dynamics to capture system behavior:
\begin{itemize}
    \item Time-delay embedding to reconstruct attractor dynamics
    \item Recurrence quantification analysis (RQA) for detecting dynamical transitions
    \item Lyapunov exponent estimation for quantifying chaotic behavior
    \item Fractal dimension calculation for measuring signal complexity
\end{itemize}

These advanced features proved particularly valuable for detecting subtle changes in machine behavior before conventional metrics showed significant deviation.

\subsection{Automated Feature Learning}
To reduce reliance on manual feature engineering, we implemented several automated approaches:

\subsubsection{Self-supervised Representation Learning}
We trained encoder networks using contrastive learning objectives:
\begin{itemize}
    \item Time-series data augmentation through jittering, scaling, and permutation
    \item Contrastive predictive coding (CPC) to learn representations that capture temporal dependencies
    \item Masked reconstruction tasks forcing the model to infer missing segments
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{figures/feature-importance.pdf}
\caption{Feature importance visualization for top 15 engineered features, highlighting the relative contribution of different sensor measurements and derived features to failure prediction accuracy.}
\label{fig:feature_importance}
\end{figure}

The resulting learned representations achieved 91\% classification accuracy using only 15\% of the labeled data required by traditional approaches.

\subsubsection{Feature Selection Optimization}
We employed advanced feature selection techniques:
\begin{itemize}
    \item Bayesian optimization for feature subset selection, which identified optimal combinations from our feature pool
    \item Genetic algorithms with multi-objective fitness functions balancing predictive power and computational complexity
    \item Stability selection to identify features robust across different operating conditions
\end{itemize}

Our optimized feature subset (43 features from an initial pool of over 200) maintained 97\% of predictive performance while reducing computational requirements by 78\%.